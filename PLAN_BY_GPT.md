Plan for Claude Code: Analyze & Enhance Camera‚ÄìWorker Delegation

# üö® IMPLEMENTATION STATUS UPDATE (2025-08-29)

## ‚úÖ COMPLETED ITEMS

### 1. Deadlock Prevention & Backend Stability (CRITICAL - RESOLVED)
- ‚úÖ **Fixed Backend Unresponsiveness**: Resolved async/sync database mixing causing deadlocks
- ‚úÖ **Task Manager**: Implemented centralized task management with connection pool awareness
- ‚úÖ **Background Task Management**: Fixed 11 locations of unhandled `asyncio.create_task()` calls
- ‚úÖ **Database Connection Pool**: Added proper limits and timeouts (10+20 async, 5+10 sync connections)
- ‚úÖ **Worker Monitor Service**: Fixed sync database usage in async contexts
- ‚úÖ **API Server**: Now starts successfully and handles concurrent requests

### 2. Enhanced Worker Architecture (IMPLEMENTED)
- ‚úÖ **HTTP Endpoints**: Workers now expose RESTful API for camera control
- ‚úÖ **Camera Streaming Service**: Full OpenCV integration with device management
- ‚úÖ **Multi-Camera Support**: Handle multiple streams with conflict resolution
- ‚úÖ **API Delegation**: Backend proxies camera operations to workers via HTTP
- ‚úÖ **Worker Registry**: Enhanced registration, heartbeat, and status tracking

### 3. Database & Models (PARTIALLY IMPLEMENTED)
- ‚úÖ **Enhanced Tables**: staff_face_images with multiple faces per staff
- ‚úÖ **RLS Implementation**: Tenant isolation with Row Level Security
- ‚úÖ **Face Processing**: Detection, landmarks, embeddings, recognition testing
- ‚ùå **camera_sessions Table**: NOT YET IMPLEMENTED (lease-based assignment missing)
- ‚ùå **Worker Capacity Fields**: Basic status tracking exists but not full capacity model

## üîÑ CHANGED FROM ORIGINAL PLAN

### Architecture Changes:
1. **Proxy Pattern**: Instead of direct WebSocket, API delegates to worker HTTP endpoints
2. **Task Manager**: Added centralized async task management (not in original plan)
3. **Multi-Face Staff**: Enhanced staff management beyond original scope
4. **Real-time Communication**: SSE + WebSocket instead of pure WebSocket protocol

### Technology Choices:
1. **OpenCV Streaming**: Direct camera access vs RTSP-only approach
2. **HTTP + REST**: Worker endpoints vs pure WebSocket communication
3. **SQLAlchemy Async**: Mixed async/sync pattern vs pure async

## ‚ö†Ô∏è REMAINING SYNC ISSUES & TECHNICAL DEBT

### Critical Remaining Issues:
1. **workers.py Router**: Still has sync database calls in async endpoints
   - `register_worker()` uses `db: Session = Depends(get_db)` with `db.commit()`
   - Multiple other endpoints mixing sync/async patterns
   - **Impact**: Potential deadlocks under heavy worker registration load

2. **Assignment Algorithm**: Using basic camera assignment vs lease-based system
   - No `camera_sessions` table implemented
   - No optimistic concurrency control
   - No soft/hard reclaim logic
   - **Impact**: Race conditions in multi-worker scenarios

3. **Worker FSM**: Basic state tracking vs formal finite state machine
   - States exist but no proper FSM implementation
   - No graceful state transitions
   - **Impact**: Unpredictable behavior during failures

## üìã IMMEDIATE PRIORITY FIXES NEEDED

### High Priority (Stability):
1. **Fix workers.py sync database calls** - Convert to async to prevent deadlocks
2. **Implement camera_sessions table** - Enable proper lease-based assignment
3. **Add worker FSM** - Proper state machine for reliability

### Medium Priority (Features):
1. **Structured logging** - Add correlation IDs and debug instrumentation
2. **Observability** - OTLP metrics and tracing
3. **Security hardening** - mTLS, encrypted RTSP credentials

### Low Priority (Optimization):
1. **Capacity-aware assignment** - Worker slot management
2. **Ring buffer** - Edge storage for replay
3. **Model hot-swap** - Dynamic model updates

---

# ORIGINAL PLAN CONTENT BELOW:

1) Inputs Claude should ask for (or autodetect)
	‚Ä¢	Codebase layout (backend FastAPI, worker service, shared protos/schemas).
	‚Ä¢	DB flavor (Postgres assumed) and migration tool (Alembic?).
	‚Ä¢	Current WebSocket protocol (messages, auth, heartbeat, errors).
	‚Ä¢	Current models/tables: tenants, sites, workers, cameras.
	‚Ä¢	Where RTSP/webcam handling lives (worker; FFmpeg/GStreamer wrappers?).
	‚Ä¢	CI pipeline & test strategy (pytest, mypy, ruff).
	‚Ä¢	Config/secrets management (env vars, Vault/KMS).

‚∏ª

1) Quick repo audit & bug triage (‚Äúconnected but not starting‚Äù)

1.1 Trace the startup path (backend & worker)
	‚Ä¢	Backend:
	‚Ä¢	Where do we assign a camera on worker connect?
	‚Ä¢	Is assignment persisted, or only in memory?
	‚Ä¢	What message instructs worker to start camera?
	‚Ä¢	Are there race conditions around site matching or camera availability?
	‚Ä¢	Worker:
	‚Ä¢	On WebSocket OPEN: do we send REGISTER? Do we await START(camera_id)?
	‚Ä¢	Do we handle START payload errors (e.g., missing RTSP creds)?
	‚Ä¢	Is there a debounce/backoff that delays start forever?

1.2 Add temporary instrumentation to isolate failure
	‚Ä¢	Backend: log (INFO) for on_connect, assignment_decision, send_start, ack_start, with correlation IDs.
	‚Ä¢	Worker: log (INFO) for on_assign, pipeline_boot, pipeline_ready, pipeline_error, heartbeat_renew.
	‚Ä¢	Verify that START is actually sent and ACKed; if not, identify where it drops (serialization, auth, filters).

1.3 Common root causes to check programmatically
	‚Ä¢	Site mismatch (worker site_id null / incorrect).
	‚Ä¢	Camera status not ‚ÄúAVAILABLE‚Äù due to stale flags.
	‚Ä¢	Assignment is ‚Äústicky‚Äù to an offline worker (no reclaim).
	‚Ä¢	Worker ignoring START due to FSM being in wrong state (e.g., RECONNECTING).
	‚Ä¢	RTSP probe failing; pipeline errors swallowed; worker never transitions to RUNNING.

Claude action: generate a ‚Äúdelegation_debug.md‚Äù checklist in the repo and wire temporary logs with structured JSON.

‚∏ª

2) Data model & migrations (introduce leases)

Create/alter tables (Postgres + Alembic):

-- camera_sessions: one row per camera; re-written per (camera_id, generation)
CREATE TABLE camera_sessions (
  camera_id         UUID PRIMARY KEY,
  tenant_id         UUID NOT NULL,
  site_id           UUID NOT NULL,
  worker_id         UUID,
  generation        BIGINT NOT NULL DEFAULT 0,
  state             TEXT NOT NULL DEFAULT 'PENDING', -- PENDING|ACTIVE|PAUSED|ORPHANED|TERMINATED
  lease_expires_at  TIMESTAMPTZ,
  reason            TEXT,
  updated_at        TIMESTAMPTZ NOT NULL DEFAULT now()
);

-- workers: add health & capacity
ALTER TABLE workers
  ADD COLUMN status TEXT NOT NULL DEFAULT 'REGISTERED',
  ADD COLUMN capacity JSONB NOT NULL DEFAULT '{"slots":1,"cpu":0,"gpu":0,"mem":0}',
  ADD COLUMN last_seen_at TIMESTAMPTZ;

-- cameras: add capabilities & last probe
ALTER TABLE cameras
  ADD COLUMN caps JSONB,           -- e.g., {"codec":"h264","res":"1920x1080","fps":25}
  ADD COLUMN last_probe_at TIMESTAMPTZ,
  ADD COLUMN last_state_change_at TIMESTAMPTZ;

Claude action: write Alembic migration with forward/backward steps and seed caps by probing known RTSP URLs (optional script).

‚∏ª

3) Control plane protocol (backend ‚Üî worker)

3.1 Message types (JSON over WebSocket; later: gRPC bidi if desired)
	‚Ä¢	Worker ‚Üí Server:
	‚Ä¢	REGISTER {worker_id, site_id, version, labels?, capacity?}
	‚Ä¢	HEARTBEAT {worker_id, metrics, renew: [{camera_id, generation}]}  // renew leases
	‚Ä¢	ACK {intent_id, status, details?}
	‚Ä¢	EVENT {camera_id, generation, seq, type, ts, payload}  // e.g., pipeline_ready, rtsp_error
	‚Ä¢	Server ‚Üí Worker:
	‚Ä¢	START {intent_id, camera_id, generation, source: {type: "rtsp"|"webcam", url_enc?|device?}, model_ver, params}
	‚Ä¢	STOP {intent_id, camera_id, generation, reason}
	‚Ä¢	RELOAD {intent_id}
	‚Ä¢	DRAIN {intent_id}

3.2 Idempotency keys
	‚Ä¢	Every server intent carries (camera_id, generation, intent_id).
	‚Ä¢	Worker emits ACK and subsequent EVENTs tagged with the same (camera_id, generation) and seq.

Claude action: generate shared Python dataclasses / Pydantic models for these messages and enforce validation.

‚∏ª

4) Assignment algorithm (lease-based & capacity-aware)

4.1 API endpoints (FastAPI)
	‚Ä¢	POST /workers/{id}/register (or via WebSocket REGISTER).
	‚Ä¢	POST /assign (internal): picks a camera for worker_id.
	‚Ä¢	Site-aware filter, tenant quota, camera status = AVAILABLE/ORPHANED.
	‚Ä¢	Capacity-aware (least slots used).
	‚Ä¢	Optimistic concurrency:

UPDATE camera_sessions
  SET worker_id=:wid, lease_expires_at=now()+:ttl, state='ACTIVE', generation=generation+1, updated_at=now()
  WHERE camera_id=:cid AND (worker_id IS NULL OR lease_expires_at < now()) AND generation=:expected_gen;

If rowcount=0 ‚Üí retry with next candidate.

	‚Ä¢	POST /leases/renew: extend lease_expires_at for the tuple(s) included in heartbeat.
	‚Ä¢	POST /leases/reclaim: cron/scheduler to mark ORPHANED when expired; HARD reclaim after 90s.

4.2 Timers
	‚Ä¢	lease_ttl: 90s (renew every 10s with heartbeat).
	‚Ä¢	hard_reclaim: >90s ‚Üí reassign immediately (simplified from original 10m window).

Claude action: implement AssignmentService with site & capacity filters + retries; unit tests for race conditions.

‚∏ª

5) Worker Finite State Machine (FSM)

States: INIT ‚Üí REGISTERED ‚Üí IDLE ‚Üí RUNNING(camera_id) ‚Üí RECONNECTING ‚Üí DRAINING ‚Üí STOPPED

Transitions:
	‚Ä¢	On START: IDLE ‚Üí RUNNING (setup pipeline, emit pipeline_ready).
	‚Ä¢	On transient net error: RUNNING ‚Üí RECONNECTING (keep buffers), then back.
	‚Ä¢	On STOP/DRAIN: gracefully stop, flush events, release lease.
	‚Ä¢	On unrecoverable RTSP error K times: emit pipeline_error, remain RUNNING but paused, or move to IDLE per policy.

Claude action: add a small FSM class (pure Python, tested) and integrate in worker main loop.

‚∏ª

6) Media pipeline hardening (worker)
	‚Ä¢	Decode via VideoToolbox (Mac mini M4) using ffmpeg or avfoundation for webcams.
	‚Ä¢	Stall detection: if no frames for N sec ‚Üí restart pipeline with jittered backoff (1, 2, 4, ‚Ä¶ 60s).
	‚Ä¢	Dynamic throttling: allow config detect_every_n_frames, downscale to 640p for detection; keep tracking internally.
	‚Ä¢	Edge ring buffer: store last 10‚Äì30s of compressed frames to /tmp/worker/{camera_id}/.
	‚Ä¢	Model hot-swap: periodically check signed bundle version; atomic reload.

Claude action: wrap pipeline in a Supervisor that produces structured EVENTs and exposes health to the main loop.

‚∏ª

7) Observability (logs, metrics, tracing)

7.1 Metrics (OTLP)
	‚Ä¢	Worker: decode_fps, detect_fps, latency_ms_p50/p95, dropped_frames, ringbuf_seconds, lease_renew_rtt, state.
	‚Ä¢	Backend: assign_attempts, assign_success, reclaims_soft/hard, leases_active, heartbeats_missing.

7.2 Structured logs
	‚Ä¢	JSON logs with tenant_id, site_id, worker_id, camera_id, generation, intent_id, trace_id.

7.3 Health rules
	‚Ä¢	Camera DOWN if no frames > X sec and K consecutive restart failures.
	‚Ä¢	Worker UNHEALTHY if missed heartbeats > M.

Claude action: instrument with OpenTelemetry (fastapi + worker), export to your collector.

‚∏ª

8) Security & privacy touch-ups
	‚Ä¢	Enforce mTLS (short-lived certs) between worker and server.
	‚Ä¢	Store RTSP creds encrypted (KMS), deliver only on START and never log.
	‚Ä¢	RBAC per tenant on all endpoints; audit log on manual overrides.
	‚Ä¢	Configurable retention for frames vs embeddings; ‚Äúdo not process‚Äù flags bypass pipeline.

Claude action: add pydantic Settings, secret loaders, and scrubbers for logs.

‚∏ª

9) Test plan (unit + integration + chaos)

9.1 Unit tests
	‚Ä¢	Assignment with optimistic concurrency & retries.
	‚Ä¢	FSM transitions (happy path, errors, drains).
	‚Ä¢	Lease renew & reclaim edge cases (time travel via freezegun).

9.2 Integration tests
	‚Ä¢	Spin backend + in-memory worker with simulated camera (file or generator).
	‚Ä¢	Validate REGISTER ‚Üí START ‚Üí pipeline_ready ‚Üí HEARTBEAT renew ‚Üí STOP ‚Üí lease release.

9.3 Chaos tests
	‚Ä¢	Drop WebSocket mid-stream; verify lease not immediately reassigned (soft window).
	‚Ä¢	Fake RTSP auth failure; confirm backoff and error event.

Claude action: create tests/integration/test_end_to_end.py and docker-compose for local run.

‚∏ª

10) Rollout plan
	1.	Phase 1 (dark launch): add camera_sessions table; write records but keep old behavior; compare assignment decisions in logs.
	2.	Phase 2: switch a small site to lease-based control; enable new worker FSM behind a feature flag.
	3.	Phase 3: enable across tenants; set SLOs (‚â•99% camera availability during business hours).
	4.	Phase 4: deprecate legacy assignment paths; remove temp logs.

Claude action: add feature flags (env or DB) and a rollout.md.

‚∏ª

11) Concrete deliverables & file map (what Claude should generate)
	‚Ä¢	Backend
	‚Ä¢	app/models/camera_sessions.py (SQLAlchemy)
	‚Ä¢	app/services/assignment.py (site/capacity/lease logic)
	‚Ä¢	app/api/ws_protocol.py (Pydantic schemas for WS messages)
	‚Ä¢	app/api/ws_handler.py (handlers, intents, ACK tracking)
	‚Ä¢	app/cron/reclaim_leases.py
	‚Ä¢	alembic/versions/<timestamp>_camera_sessions.py
	‚Ä¢	app/observability/otlp.py
	‚Ä¢	Worker
	‚Ä¢	worker/fsm.py
	‚Ä¢	worker/protocol.py (schemas; shared if possible)
	‚Ä¢	worker/supervisor.py (pipeline wrapper, ring buffer, backoff)
	‚Ä¢	worker/main.py (WS loop: register ‚Üí heartbeat ‚Üí handle intents)
	‚Ä¢	worker/observability/metrics.py
	‚Ä¢	Tests
	‚Ä¢	tests/unit/test_assignment.py
	‚Ä¢	tests/unit/test_fsm.py
	‚Ä¢	tests/integration/test_e2e.py
	‚Ä¢	Docs
	‚Ä¢	docs/protocol.md
	‚Ä¢	docs/delegation_debug.md
	‚Ä¢	docs/rollout.md

‚∏ª

12) Pseudocode snippets Claude can expand

12.1 Assign camera (optimistic concurrency)

async def assign_camera(worker_id: UUID, site_id: UUID) -> Optional[CameraSession]:
    candidates = await repo.list_available_cameras(site_id=site_id)
    for cam in sorted(candidates, key=score_by_capacity):
        # read current generation
        sess = await repo.get_or_create_session(cam.id)
        updated = await repo.try_take_lease(
            camera_id=cam.id,
            worker_id=worker_id,
            expect_generation=sess.generation,
            ttl=timedelta(seconds=90),
        )
        if updated: 
            await control.send_start(worker_id, cam, generation=sess.generation+1)
            return updated
    return None

12.2 Heartbeat & renew

# worker
while True:
    await ws.send_json({
        "type":"HEARTBEAT",
        "worker_id": wid,
        "metrics": collect_metrics(),
        "renew": [{"camera_id": cid, "generation": gen} for cid, gen in active_sessions()],
    })
    await asyncio.sleep(10)

12.3 FSM sketch

class WorkerFSM:
    state = "IDLE"
    def on_start(self, camera):
        assert self.state in {"IDLE","RECONNECTING"}
        self.state = "RUNNING"
        self.supervisor.start(camera)
    def on_stop(self):
        if self.state == "RUNNING":
            self.supervisor.stop()
        self.state = "IDLE"


‚∏ª

13) Acceptance criteria (definition of done)
	‚Ä¢	Worker connects; within ‚â§3s receives START when an eligible camera exists.
	‚Ä¢	Pipeline emits pipeline_ready within ‚â§10s, backend marks camera ACTIVE.
	‚Ä¢	Lease renews every 10s; if worker disconnects:
	‚Ä¢	>90s: hard reclaim; different worker starts camera within ‚â§20s.
	‚Ä¢	Observability: dashboard shows per-camera state, lease TTL, worker health.
	‚Ä¢	No plaintext RTSP creds in logs. All intents/event logs include correlation IDs.
	‚Ä¢	Test suite passes locally and in CI; integration test covers the above flows.

‚∏ª

14) Immediate fix for ‚Äúconnected but does not start‚Äù

Claude should:
	1.	Add structured logs around REGISTER, assignment, send START, ACK START.
	2.	Assert that site_id is present on workers and used in filters.
	3.	Verify backend actually persists an assignment (or new camera_sessions) and sends START.
	4.	On worker, ensure START handler calls the pipeline supervisor and emits pipeline_ready/pipeline_error.
	5.	Add a timeout guard: if START sent but no pipeline_ready within 15s, backend marks session PAUSED with reason and optionally retries elsewhere (respecting soft window).
